{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e797d86-81bf-41ba-811f-3dd517d004e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Biomedical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8ab26-812d-4b72-b378-01e78586b752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "                ┌────────────────────┐\n",
    "                │   User Query       │\n",
    "                └────────┬───────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌────────────────────┐\n",
    "                │   Embed Query      │ ◄─ Using dense embedding model (e.g., SBERT, OpenAI)\n",
    "                └────────┬───────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌────────────────────┐\n",
    "                │ Vector Search (kNN)│ ◄─ In vector store (e.g., FAISS, Pinecone)\n",
    "                └────────┬───────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌────────────────────┐\n",
    "                │ Retrieve Top-k Docs│\n",
    "                └────────┬───────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌────────────────────┐\n",
    "                │  Format Context    │\n",
    "                └────────┬───────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌────────────────────┐\n",
    "                │  Prompt LLM (RAG)  │ ◄─ Append query + context\n",
    "                └────────┬───────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                ┌────────────────────┐\n",
    "                │   Generated Answer │\n",
    "                └────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770e52a9-4152-4628-8c5e-ec384d56e630",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading data/my_knowledge.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python-envs/casstle_env/lib/python3.13/site-packages/langchain_community/document_loaders/text.py:42\u001b[39m, in \u001b[36mTextLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     43\u001b[39m         text = f.read()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/my_knowledge.txt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 1. Load and split your documents\u001b[39;00m\n\u001b[32m     10\u001b[39m loader = TextLoader(\u001b[33m\"\u001b[39m\u001b[33mdata/my_knowledge.txt\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# or use DirectoryLoader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m documents = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m text_splitter = CharacterTextSplitter(chunk_size=\u001b[32m500\u001b[39m, chunk_overlap=\u001b[32m50\u001b[39m)\n\u001b[32m     14\u001b[39m chunks = text_splitter.split_documents(documents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python-envs/casstle_env/lib/python3.13/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python-envs/casstle_env/lib/python3.13/site-packages/langchain_community/document_loaders/text.py:58\u001b[39m, in \u001b[36mTextLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     60\u001b[39m metadata = {\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path)}\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m Document(page_content=text, metadata=metadata)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error loading data/my_knowledge.txt"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# 1. Load and split your documents\n",
    "loader = TextLoader(\"data/my_knowledge.txt\")  # or use DirectoryLoader\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 2. Embed documents and create vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# 3. Define the retriever\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", k=4)\n",
    "\n",
    "# 4. Setup Ollama LLM\n",
    "llm = Ollama(model=\"llama3:instruct\", temperature=0)\n",
    "\n",
    "# 5. Create RAG chain \n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 5. Ask a question\n",
    "query = \"How does photosynthesis work?\"\n",
    "result = rag_chain(query)\n",
    "\n",
    "# 6. Show result\n",
    "print(\"Answer:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1e6e1-4a23-40b6-9889-706f0bc267fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casstle",
   "language": "python",
   "name": "casstle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
